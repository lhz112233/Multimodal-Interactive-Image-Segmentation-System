import gradio as gr
import torch
import clip
from segment_anything import build_sam, SamAutomaticMaskGenerator
import numpy as np
from PIL import Image, ImageDraw
import os
from datetime import datetime
import pandas as pd
import requests
import threading
import tempfile
import matplotlib.pyplot as plt


# ========== å…¨å±€å˜é‡ ==========
stop_event = threading.Event()
history_records = []
upload_mode = ""

# ========== åˆå§‹åŒ–æ¨¡å— ==========
# åˆå§‹åŒ–è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å½“å‰è®¾å¤‡: {torch.cuda.get_device_name(torch.cuda.current_device())}")
# åŠ è½½åˆ†å‰²æ¨¡å‹
def load_segmentation_models():
    print('å¼€å§‹åŠ è½½CLIPæ¨¡å‹...')
    clip_model, preprocess = clip.load("ViT-B/32", device=device)
    print('CLIPæ¨¡å‹åŠ è½½å®Œæˆ,è®¾å¤‡ï¼š', device, ' æ¨¡å‹ï¼š', 'ViT-B/32')    
    print('å¼€å§‹åŠ è½½SAMæ¨¡å‹...')
    model_path = os.path.join(os.path.dirname(__file__), "path/model3.pth")
    if os.path.exists(model_path):
        sam = build_sam(checkpoint=model_path)
        # sam.to(device)
        mask_generator = SamAutomaticMaskGenerator(sam)
        print('SAMæ¨¡å‹åŠ è½½å®Œæˆ')
        return clip_model, preprocess, mask_generator
    else:
        raise FileNotFoundError(f"Model file not found at {model_path}")

clip_model, preprocess, mask_generator = load_segmentation_models()

# ========== å›¾åƒå¤„ç†æ¨¡å— ==========
def convert_box_xywh_to_xyxy(box):
    return [box[0], box[1], box[0] + box[2], box[1] + box[3]]

def segment_image(image, segmentation_mask):
    image_array = np.array(image)
    segmented_image_array = np.zeros_like(image_array)
    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]
    segmented_image = Image.fromarray(segmented_image_array)
    black_image = Image.new("RGB", image.size, (0, 0, 0))
    transparency_mask = Image.fromarray(segmentation_mask.astype(np.uint8) * 255)
    black_image.paste(segmented_image, mask=transparency_mask)
    return black_image

@torch.no_grad()
def retrieve_images(elements: list, search_terms: list, process_info):
    start_time = datetime.now()

    # é¢„å¤„ç†æ‰€æœ‰å›¾åƒ
    preprocessed_images = [preprocess(img).to(device) for img in elements]
    stacked_images = torch.stack(preprocessed_images)

    # ç¼–ç æ–‡æœ¬
    tokenized_text = clip.tokenize(search_terms).to(device)

    # æå–ç‰¹å¾
    image_features = clip_model.encode_image(stacked_images)
    text_features = clip_model.encode_text(tokenized_text)

    # å½’ä¸€åŒ–ç‰¹å¾
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # è®¡ç®—ç›¸ä¼¼åº¦å¹¶å–æœ€å¤§å€¼
    probs = 100.0 * (image_features @ text_features.T)
    max_probs, _ = torch.max(probs, dim=1)
    scores = max_probs.softmax(dim=0)

    process_info["clip_time"] = (datetime.now() - start_time).total_seconds()
    return scores

def createOverlay(image, masks, indices):
    original_image = image.convert("RGBA")
    overlay_image = Image.new("RGBA", original_image.size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(overlay_image)
    for seg_idx in indices:
        mask = masks[seg_idx]["segmentation"]
        mask_image = Image.fromarray(mask.astype(np.uint8) * 255)
        draw.bitmap((0, 0), mask_image, fill=(240, 135, 132, 200))
    return Image.alpha_composite(original_image, overlay_image)

def save_temp_file(image: Image.Image):
    temp_dir = tempfile.mkdtemp()
    temp_path = os.path.join(temp_dir, "temp_image.png")
    image.save(temp_path)
    return temp_path

def process_image_batch(input_images, input_webcam, upload_mode, search_text, show_mask, threshold, box_nms_thresh):    
    stop_event.clear()
    process_info = {"timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    mask_generator.box_nms_thresh = box_nms_thresh
    
    # ç»Ÿä¸€å¤„ç†å›¾åƒæº
    if upload_mode == "æœ¬åœ°ä¸Šä¼ ":
        image_list = input_images
    else:
        image_list = [save_temp_file(input_webcam)]
    
    # åˆ†å‰²æœç´¢å…³é”®è¯
    search_terms = [term.strip() for term in search_text.split(',') if term.strip()]
    if not search_terms:
        return [None] * len(image_list), pd.DataFrame(), "æœªè¾“å…¥æœ‰æ•ˆæœç´¢å…³é”®è¯"

    print(f"upload_mode: {upload_mode}, image_list: {image_list}, search_text: {search_text}")

    result_images = []
    all_logs = []
    
    for idx, image_file in enumerate(image_list, 1):
        if stop_event.is_set():
            all_logs.append("å¤„ç†å·²ç”±ç”¨æˆ·ç»ˆæ­¢")
            break
        
        try:
            # ç›´æ¥æ‰“å¼€ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶
            # image = Image.open(image_file) if isinstance(image_file, str) else image_file
            image = Image.open(image_file)
            image_rgb = image.convert("RGB")
            image_np = np.array(image_rgb)
            
            # ç”Ÿæˆåˆ†å‰²æ©ç 
            masks = mask_generator.generate(image_np)
            cropped_boxes = [segment_image(image_rgb, m["segmentation"]).crop(convert_box_xywh_to_xyxy(m["bbox"]))
                            for m in masks]
            
            # æ£€ç´¢åŒ¹é…åŒºåŸŸ
            scores = retrieve_images(cropped_boxes, search_terms, process_info)
            indices = [i for i, v in enumerate(scores) if v > threshold]

            # ç”Ÿæˆç»“æœå›¾åƒ
            result_img = createOverlay(image_rgb, masks, indices) if show_mask else image_rgb
            result_images.append(result_img)

            log = f"å›¾ç‰‡{idx}: æ£€æµ‹åˆ°{len(indices)}ä¸ªåŒ¹é…åŒºåŸŸ | è€—æ—¶{process_info['clip_time']:.2f}s"
            all_logs.append(log)
            
            # æ›´æ–°å†å²è®°å½•
            global history_records
            history_records.append([
                process_info["timestamp"], search_text, threshold, len(indices), process_info["clip_time"]
            ])

        except Exception as e:
            all_logs.append(f"å›¾ç‰‡{idx}å¤„ç†å¤±è´¥: {str(e)}")
            result_images.append(None)
    
    log_df = pd.DataFrame({"å¤„ç†æ—¥å¿—": all_logs})
    stats = f"å…±å¤„ç†{len(result_images)}å¼ å›¾ç‰‡ | æ€»è€—æ—¶{sum([process_info.get('clip_time', 0)]):.2f}ç§’" 
    history_df = pd.DataFrame(history_records, columns=["æ—¶é—´", "æœç´¢å†…å®¹", "é˜ˆå€¼", "åŒ¹é…æ•°", "è€—æ—¶"])   
    
    return result_images, log_df, stats, history_df

def stop_processing():
    stop_event.set()

# ========== ç•Œé¢æ¨¡å— ==========
# ä¿®æ”¹å›¾åƒåˆ†å‰²ç•Œé¢åˆ›å»ºå‡½æ•°
def create_image_segmentation_interface():
    with gr.Blocks() as interface:
        with gr.Row():
            input_col = gr.Column(scale=3)
            output_col = gr.Column(scale=7)
            with input_col:
                with gr.Group():
                    # æ–°å¢ä¸Šä¼ æ¨¡å¼é€‰æ‹©
                    upload_mode = gr.Radio(
                        choices=["æœ¬åœ°ä¸Šä¼ ", "æ‹ç…§ä¸Šä¼ "],
                        label="ä¸Šä¼ æ¨¡å¼",
                        value="æœ¬åœ°ä¸Šä¼ "
                    )
                    
                    # æœ¬åœ°ä¸Šä¼ ç»„ä»¶
                    input_images = gr.Files(label="ä¸Šä¼ å›¾ç‰‡", file_types=["image"], file_count="multiple")
                    # æ‹ç…§ä¸Šä¼ ç»„ä»¶
                    input_webcam = gr.Image(label="æ‹ç…§ä¸Šä¼ ", sources="webcam", type="pil", show_download_button=True, visible=False)
                    # ç”»å»Šç»„ä»¶
                    input_gallery = gr.Gallery(label="ä¸Šä¼ å›¾ç‰‡é¢„è§ˆ", columns=3, height=300)
                    # åŠ¨æ€åˆ‡æ¢ä¸Šä¼ ç»„ä»¶
                    def toggle_upload_mode(mode):
                        return {
                            input_images: gr.update(visible=mode == "æœ¬åœ°ä¸Šä¼ "),
                            input_webcam: gr.update(visible=mode == "æ‹ç…§ä¸Šä¼ "),
                        }
                    
                    upload_mode.change(
                        fn=toggle_upload_mode,
                        inputs=upload_mode,
                        outputs=[input_images, input_webcam]
                    )

                    # é¢„è§ˆä¸Šä¼ å›¾ç‰‡
                    input_images.change(
                        fn=lambda files: [Image.open(f) for f in files] if files else [],
                        inputs=input_images,
                        outputs=input_gallery
                    )
                    
                    input_webcam.change(
                        fn=lambda img: [img] if img is not None else [],
                        inputs=input_webcam,
                        outputs=input_gallery
                    )
                    
                    # æ·»åŠ ç¤ºä¾‹æç¤ºè¯
                    example_prompts = [
                        ["horse, fish"],
                        ["dog, cat"],
                        ["car, bicycle, airplane"],
                        ["person, building, sky"],
                        ["flower, tree, mountain"],
                        ["sun, moon, stars"],
                        ["food, drink, clothes"],
                        ["fruit, vegetable, animal"],
                    ]
                    
                   
                    search_text = gr.Textbox(label="æœç´¢å†…å®¹", placeholder="eg: person, dog, car, ...")
                    
                    # æ·»åŠ ç‚¹å‡»ç¤ºä¾‹
                    gr.Examples(
                        label="æœç´¢æ¨¡æ¿",
                        examples=example_prompts,
                        inputs=[search_text],
                        examples_per_page=3,
                    )
                    
                    with gr.Row():
                        threshold = gr.Slider(0.01, 0.5, value=0.1, label="åŒ¹é…é˜ˆå€¼")
                        box_nms_thresh = gr.Slider(0.1, 0.9, value=0.7, label="åŒºåŸŸåˆå¹¶é˜ˆå€¼")
                        
                    
                    stop_btn = gr.Button("ğŸ›‘ ç»ˆæ­¢å¤„ç†", variant="stop")
                    process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")

            with output_col:
                with gr.Tabs():
                    with gr.Tab("ç»“æœå±•ç¤º"):
                        output_gallery = gr.Gallery(label="å¤„ç†ç»“æœ", columns=3, height=800)
                    with gr.Tab("å¤„ç†æ—¥å¿—"):
                        logs = gr.Dataframe(label="æ—¥å¿—è®°å½•", headers=["å¤„ç†è¯¦æƒ…"])
                        stats = gr.Textbox(label="ç»Ÿè®¡ä¿¡æ¯")
                    with gr.Tab("å†å²è®°å½•"):
                        history_table = gr.Dataframe(
                            headers=["æ—¶é—´", "æœç´¢å†…å®¹", "é˜ˆå€¼", "åŒ¹é…æ•°", "è€—æ—¶"],
                            datatype=["str", "str", "number", "number", "number"],
                            interactive=False,
                        height=400
                        )
                        # æ¸…é™¤å†å²æŒ‰é’®
                        clear_history_btn = gr.Button("ğŸ—‘ æ¸…é™¤å†å²è®°å½•", variant="secondary")
                        # ç»‘å®šæ¸…é™¤åŠŸèƒ½
                        def clear_history():
                            global history_records
                            history_records = []
                            return pd.DataFrame(columns=["æ—¶é—´", "æœç´¢å†…å®¹", "é˜ˆå€¼", "åŒ¹é…æ•°", "è€—æ—¶"])
      
        clear_history_btn.click(
            fn=clear_history,
            inputs=None,
            outputs=history_table
        )
            
        process_btn.click(
            fn=process_image_batch,
            inputs=[
                input_images,    # å›¾åƒæº1
                input_webcam,    # å›¾åƒæº2
                upload_mode,     # ä¸Šä¼ æ¨¡å¼
                search_text,     # æœç´¢å†…å®¹
                gr.Checkbox(True, visible=False),# å¼ºåˆ¶æ˜¾ç¤ºæ©ç ç»“æœ 
                threshold,       # åŒ¹é…é˜ˆå€¼
                box_nms_thresh   # åŒºåŸŸåˆå¹¶é˜ˆå€¼
            ],
            outputs=[output_gallery, logs, stats, history_table]
        )
        
        # if upload_mode == "æœ¬åœ°ä¸Šä¼ ":
        #     process_btn.click(
        #     fn=process_image_batch,
        #     inputs=[
        #         input_images,    # å›¾åƒæº
        #         upload_mode,     # ä¸Šä¼ æ¨¡å¼
        #         search_text,     # æœç´¢å†…å®¹
        #         gr.Checkbox(True, visible=False),# å¼ºåˆ¶æ˜¾ç¤ºæ©ç ç»“æœ 
        #         threshold,       # åŒ¹é…é˜ˆå€¼
        #         box_nms_thresh   # åŒºåŸŸåˆå¹¶é˜ˆå€¼
        #     ],
        #     outputs=[output_gallery, logs, stats, history_table]
        # )
        # else:  # æ‹ç…§ä¸Šä¼ æ¨¡å¼
        #     process_btn.click(
        #     fn=process_image_batch,
        #     inputs=[
        #         input_webcam,    # å›¾åƒæº
        #         upload_mode,     # ä¸Šä¼ æ¨¡å¼
        #         search_text,     # æœç´¢å†…å®¹
        #         gr.Checkbox(True, visible=False),# å¼ºåˆ¶æ˜¾ç¤ºæ©ç ç»“æœ 
        #         threshold,       # åŒ¹é…é˜ˆå€¼
        #         box_nms_thresh   # åŒºåŸŸåˆå¹¶é˜ˆå€¼
        #     ],
        #     outputs=[output_gallery, logs, stats, history_table]
        # )
        
        
        stop_btn.click(fn=lambda: stop_event.set(), inputs=None, outputs=None)
        # stop_btn.click(
        #     fn=process_image_batch,
        #     inputs=[],
        #     outputs=[]
        # )
        
    return interface

# ========== èŠå¤©æ¨¡å— ==========
DEEPSEEK_API_KEY = "sk-03b1f58d372240078828fc3c47482ca6"
API_URL = "https://api.deepseek.com/v1/chat/completions"

def doChatbot(message, history):
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé›†æˆåŒ–å¤šæ¨¡æ€å›¾åƒåˆ†æç³»ç»Ÿ å·²å®Œæˆæ ¸å¿ƒç®—æ³•ä¸å…¨æ ˆäº¤äº’é—­ç¯çš„æ„å»º 1 é€šè¿‡èåˆSAMæ¨¡å‹çš„ç²¾å‡†åˆ†å‰²èƒ½åŠ›ä¸CLIPçš„è·¨æ¨¡æ€è¯­ä¹‰ç†è§£ å®ç°ç‰©ä½“çº§è‡ªåŠ¨åˆ†å‰² å¤šæ ‡ç­¾åˆ†ç±»åŠåŠ¨æ€æ©ç å¯è§†åŒ– ç»“åˆå¼‚æ­¥æ‰¹å¤„ç†æŠ€æœ¯è¾¾åˆ°é«˜æ•ˆå¹¶è¡Œè¿ç®— 2 åŸºäºDeepSeekå¤§æ¨¡å‹æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å‹AIåŠ©æ‰‹ æ”¯æŒå‚æ•°è°ƒä¼˜å»ºè®®ä¸ç»“æœæ™ºèƒ½è§£æ é…åˆæ»‘åŠ¨å¼é˜ˆå€¼è°ƒèŠ‚å’ŒNMSåˆå¹¶ç³»æ•°é…ç½®é¢æ¿å½¢æˆè‡ªé€‚åº”å·¥ä½œæµ 3 é‡‡ç”¨Gradioæ¡†æ¶æ‰“é€ ä¸‰æ¨¡æ€æ“ä½œç•Œé¢ é›†æˆåŠ¨æ€ç¼©ç•¥å›¾é¢„è§ˆ åŒç»´åº¦æ—¥å¿—è¿½è¸ªåŠåˆ†å‰²-æ£€ç´¢-å¯¹è¯ååŒå·¥ä½œåŒº å®ç°ä¸Šä¼ -åˆ†æ-äº¤äº’çš„å…¨æµç¨‹å¯è§†åŒ–é—­ç¯ ç³»ç»Ÿå¹³å‡å¤„ç†æ—¶æ•ˆä¼˜äº2ç§’/å›¾ æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å›¾åƒåˆ†æå·¥ç¨‹çš„æ™ºèƒ½åŒ–ä¸æ˜“ç”¨æ€§"}]
    
    for user, bot in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})
    messages.append({"role": "user", "content": message})

    try:
        response = requests.post(API_URL, headers=headers, json={
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.7
        }, timeout=30)
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"è¯·æ±‚å‡ºé”™ï¼š{str(e)}"

# ========== ä¸»ç•Œé¢ ==========
html_content = """
<div style='
    display: flex; 
    justify-content: center; /* æ°´å¹³å±…ä¸­ */
    align-items: center; /* å‚ç›´å±…ä¸­ */
    height: 100px; /* è®¾ç½®å—çš„é«˜åº¦ */
    border-radius: 10px; /* åœ†è§’ï¼ˆå¯é€‰ï¼‰ */
    font-size: 44px; /* å­—ä½“å¤§å° */
    text-align: center; /* æ–‡æœ¬å±…ä¸­ */
'>
    å¤šæ¨¡æ€äº¤äº’å¼å›¾åƒåˆ†å‰²ç³»ç»Ÿ 2.0
</div>
"""


def create_main_interface():
    with gr.Blocks(title="æ¸©æµ©2021218526", theme=gr.themes.Soft()) as demo:
        with gr.Row():
            gr.Image("assets/logo.png",
                     height=100,
                     show_download_button=False,
                     show_fullscreen_button=False,
                     show_label=False)
        with gr.Row():
            gr.HTML(html_content)
        with gr.Tabs():
            with gr.Tab("ğŸ“· å›¾åƒåˆ†å‰²"):
                create_image_segmentation_interface()
            
            # with gr.Tab("ğŸ¨ æ–‡ç”Ÿå›¾"):
            #     with gr.Row():
            #         with gr.Column(scale=15):
            #             txt1 = gr.Textbox(lines=2, label="")
            #             txt2 = gr.Textbox(lines=2, label="")
            #         with gr.Column(scale=1,min_width=1):
            #             button1 = gr.Button(value="1")
            #             button2 = gr.Button(value="2")
            #             button3 = gr.Button(value="3")
            #             button4 = gr.Button(value="4")
            #         with gr.Column(scale=6):
            #             generate_button = gr.Button(value="Generate",variant="primary",scale=1)
            #             with gr.Row():
            #                 dropdown = gr.Dropdown(["1", "2", "3", "4"], label="Style1")
            #                 dropdown2 = gr.Dropdown(["1", "2", "3", "4"], label="Style2")

            #     with gr.Row():
            #         with gr.Column():
            #             with gr.Row():
            #                 dropdown3 = gr.Dropdown(["1", "2", "3", "4"], label="Sampling method")
            #                 slider1 = gr.Slider(minimum=0, maximum=1, label="Sampling steps")
            #             checkboxgroup = gr.CheckboxGroup(["Restore faces", "Tiling", "Hires.fix"], label="")
            #             with gr.Row():
            #                 slider2 = gr.Slider(minimum=0, maximum=100, label="Width")
            #                 slider3 = gr.Slider(minimum=0, maximum=100, label="Batch count")
            #             with gr.Row():
            #                 slider4 = gr.Slider(minimum=0, maximum=100, label="Height")
            #                 slider5 = gr.Slider(minimum=0, maximum=100, label="Batch size")
            #             slider6 = gr.Slider(minimum=0, maximum=100, label="CFG scale")
            #             with gr.Row():
            #                 number = gr.Number(label="Seed")
            #                 button5 = gr.Button(value="Randomize")
            #                 button6 = gr.Button(value="Reset")
            #                 checkbox1 = gr.Checkbox(label="Extra")
            #             dropdown4 = gr.Dropdown(["1", "2", "3", "4"], label="Script")
            #         with gr.Column():
            #             gallery = gr.Gallery([],columns=3)
            #             with gr.Row():
            #                 button66 = gr.Button(value="Save")
            #                 button7 = gr.Button(value="Save as")
            #                 button8 = gr.Button(value="Zip")
            #                 button9 = gr.Button(value="Send to img2img",min_width=1)
            #                 button10 = gr.Button(value="Send to inpaint",min_width=1)
            #                 button11 = gr.Button(value="Send to extras",min_width=1)
            #             txt3 = gr.Textbox(lines=4, label="")

            with gr.Tab("ğŸ’¬ AIåŠ©æ‰‹"):
                gr.ChatInterface(
                    fn=doChatbot,
                    chatbot=gr.Chatbot(height=500, bubble_full_width=False),
                    textbox=gr.Textbox(placeholder="è¾“å…¥å…³äºå›¾åƒå¤„ç†çš„é—®é¢˜...", container=False),
                    title="ğŸ¤– AIåŠ©æ‰‹",
                    examples=[
                        ["å¦‚ä½•æé«˜åˆ†å‰²ç²¾åº¦ï¼Ÿ"],
                        ["å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ"],
                        ["è§£é‡Šä¸‹åŒ¹é…é˜ˆå€¼çš„ä½œç”¨"]
                    ],
                    submit_btn="å‘é€",
                    retry_btn="é‡è¯•",
                    undo_btn="æ’¤å›",
                    clear_btn="æ¸…ç©º"
                )

    return demo

# ========== è¿è¡Œç¨‹åº ==========
if __name__ == "__main__":
    app = create_main_interface()
    app.launch(
        server_port=8000,
        share=True,
        # favicon="assets/favicon.ico",
        auth=("wh2021218526", "hfut123456")  # è´¦å·andå¯†ç 
    )